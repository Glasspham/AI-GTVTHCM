# HÆ°á»›ng dáº«n Chá»‰nh SiÃªu Tham Sá»‘

## ğŸ“ Vá»‹ trÃ­ chá»‰nh sá»­a

Má»Ÿ file `titanic_comparison.py` vÃ  tÃ¬m pháº§n **Cáº¤U HÃŒNH SIÃŠU THAM Sá»** (dÃ²ng 15-45)

## âš™ï¸ CÃ¡c siÃªu tham sá»‘ cÃ³ thá»ƒ chá»‰nh

### 1. **hidden_layers** - Kiáº¿n trÃºc máº¡ng

Sá»‘ lÆ°á»£ng neurons trong má»—i hidden layer.

```python
# VÃ­ dá»¥:
'hidden_layers': (64, 32)           # 2 layers: 64 vÃ  32 neurons
'hidden_layers': (128, 64, 32)      # 3 layers
'hidden_layers': (100,)             # 1 layer vá»›i 100 neurons
'hidden_layers': (256, 128, 64, 32) # 4 layers
```

**Khuyáº¿n nghá»‹:**

- ANN (Shallow): 1-3 layers, má»—i layer 32-128 neurons
- Deep Learning: 4-7 layers, má»—i layer 16-256 neurons

---

### 2. **activation** - HÃ m kÃ­ch hoáº¡t

HÃ m phi tuyáº¿n giá»¯a cÃ¡c layer.

```python
# CÃ¡c lá»±a chá»n:
'activation': 'relu'      # âœ… Tá»‘t nháº¥t cho háº§u háº¿t trÆ°á»ng há»£p
'activation': 'tanh'      # Tá»‘t cho dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a
'activation': 'logistic'  # Sigmoid, cháº­m hÆ¡n relu
```

**Khuyáº¿n nghá»‹:** DÃ¹ng `'relu'` (máº·c Ä‘á»‹nh)

---

### 3. **learning_rate** - Chiáº¿n lÆ°á»£c learning rate

```python
# CÃ¡c lá»±a chá»n:
'learning_rate': 'adaptive'    # âœ… Tá»± Ä‘á»™ng giáº£m khi khÃ´ng cáº£i thiá»‡n
'learning_rate': 'constant'    # Giá»¯ nguyÃªn learning rate
'learning_rate': 'invscaling'  # Giáº£m dáº§n theo cÃ´ng thá»©c
```

**Khuyáº¿n nghá»‹:** DÃ¹ng `'adaptive'` (máº·c Ä‘á»‹nh)

---

### 4. **learning_rate_init** - Learning rate ban Ä‘áº§u

Tá»‘c Ä‘á»™ há»c cá»§a mÃ´ hÃ¬nh.

```python
# VÃ­ dá»¥:
'learning_rate_init': 0.001   # âœ… Máº·c Ä‘á»‹nh, tá»‘t cho háº§u háº¿t
'learning_rate_init': 0.01    # Há»c nhanh hÆ¡n (cÃ³ thá»ƒ bá» qua minimum)
'learning_rate_init': 0.0001  # Há»c cháº­m hÆ¡n (á»•n Ä‘á»‹nh hÆ¡n)
```

**Khuyáº¿n nghá»‹:**

- Báº¯t Ä‘áº§u vá»›i `0.001`
- Náº¿u loss dao Ä‘á»™ng: giáº£m xuá»‘ng `0.0001`
- Náº¿u há»c quÃ¡ cháº­m: tÄƒng lÃªn `0.01`

---

### 5. **max_iter** - Sá»‘ epochs tá»‘i Ä‘a

```python
# VÃ­ dá»¥:
'max_iter': 500    # âœ… Máº·c Ä‘á»‹nh
'max_iter': 1000   # Cho phÃ©p huáº¥n luyá»‡n lÃ¢u hÆ¡n
'max_iter': 200    # Huáº¥n luyá»‡n nhanh (cÃ³ thá»ƒ chÆ°a há»™i tá»¥)
```

**Khuyáº¿n nghá»‹:**

- Dataset nhá»: 200-500
- Dataset lá»›n: 500-1000

---

### 6. **alpha** - L2 Regularization

Giáº£m overfitting báº±ng cÃ¡ch pháº¡t weights lá»›n.

```python
# VÃ­ dá»¥:
'alpha': 0.0001    # âœ… Máº·c Ä‘á»‹nh, regularization nháº¹
'alpha': 0.001     # Regularization máº¡nh hÆ¡n (giáº£m overfitting)
'alpha': 0.00001   # Regularization yáº¿u hÆ¡n
'alpha': 0         # KhÃ´ng regularization
```

**Khuyáº¿n nghá»‹:**

- Náº¿u overfitting (train acc >> test acc): tÄƒng alpha lÃªn `0.001` hoáº·c `0.01`
- Náº¿u underfitting: giáº£m alpha xuá»‘ng `0.00001` hoáº·c `0`

---

### 7. **batch_size** - KÃ­ch thÆ°á»›c batch

```python
# VÃ­ dá»¥:
'batch_size': 'auto'  # âœ… Máº·c Ä‘á»‹nh, tá»± Ä‘á»™ng = min(200, n_samples)
'batch_size': 32      # Batch nhá» (cáº­p nháº­t weights thÆ°á»ng xuyÃªn)
'batch_size': 64      # Batch trung bÃ¬nh
'batch_size': 128     # Batch lá»›n (huáº¥n luyá»‡n nhanh hÆ¡n)
```

**Khuyáº¿n nghá»‹:**

- Dataset nhá» (<1000): `'auto'` hoáº·c `32`
- Dataset lá»›n: `64` hoáº·c `128`

---

## ğŸ”§ VÃ­ dá»¥ Cáº¥u HÃ¬nh

### Cáº¥u hÃ¬nh 1: TÄƒng Ä‘á»™ phá»©c táº¡p ANN

```python
ANN_CONFIG = {
    'hidden_layers': (128, 64, 32),     # ThÃªm 1 layer
    'activation': 'relu',
    'learning_rate': 'adaptive',
    'learning_rate_init': 0.001,
    'max_iter': 500,
    'alpha': 0.0001,
    'batch_size': 'auto',
}
```

### Cáº¥u hÃ¬nh 2: Giáº£m overfitting

```python
DEEP_CONFIG = {
    'hidden_layers': (128, 64, 64, 32, 16),
    'activation': 'relu',
    'learning_rate': 'adaptive',
    'learning_rate_init': 0.001,
    'max_iter': 500,
    'alpha': 0.01,                      # TÄƒng regularization
    'batch_size': 64,                   # Batch lá»›n hÆ¡n
}
```

### Cáº¥u hÃ¬nh 3: Há»c cháº­m vÃ  á»•n Ä‘á»‹nh

```python
ANN_CONFIG = {
    'hidden_layers': (64, 32),
    'activation': 'relu',
    'learning_rate': 'adaptive',
    'learning_rate_init': 0.0001,       # Learning rate nhá»
    'max_iter': 1000,                   # Nhiá»u epochs hÆ¡n
    'alpha': 0.0001,
    'batch_size': 32,                   # Batch nhá»
}
```

### Cáº¥u hÃ¬nh 4: Thá»­ activation function khÃ¡c

```python
DEEP_CONFIG = {
    'hidden_layers': (128, 64, 64, 32, 16),
    'activation': 'tanh',               # Thá»­ tanh thay vÃ¬ relu
    'learning_rate': 'adaptive',
    'learning_rate_init': 0.001,
    'max_iter': 500,
    'alpha': 0.0001,
    'batch_size': 'auto',
}
```

---

## ğŸ“Š CÃ¡ch Thá»­ Nghiá»‡m

1. **Chá»‰nh 1 tham sá»‘ táº¡i 1 thá»i Ä‘iá»ƒm** Ä‘á»ƒ biáº¿t tÃ¡c Ä‘á»™ng cá»§a nÃ³
2. **Ghi láº¡i káº¿t quáº£** sau má»—i láº§n cháº¡y
3. **So sÃ¡nh Test Accuracy** Ä‘á»ƒ chá»n cáº¥u hÃ¬nh tá»‘t nháº¥t

### Quy trÃ¬nh thá»­ nghiá»‡m:

```bash
# 1. Chá»‰nh cáº¥u hÃ¬nh trong file
# 2. Cháº¡y thá»­ nghiá»‡m
python titanic_comparison.py

# 3. Ghi láº¡i káº¿t quáº£
# 4. Thá»­ cáº¥u hÃ¬nh khÃ¡c
```

---

## ğŸ¯ Má»¥c TiÃªu Tá»‘i Æ¯u

### Náº¿u muá»‘n tÄƒng Test Accuracy:

1. Thá»­ tÄƒng sá»‘ neurons: `(128, 64)` â†’ `(256, 128)`
2. Thá»­ thÃªm layers: `(64, 32)` â†’ `(64, 32, 16)`
3. Giáº£m learning rate: `0.001` â†’ `0.0001`
4. TÄƒng epochs: `500` â†’ `1000`

### Náº¿u bá»‹ Overfitting (Train >> Test):

1. TÄƒng regularization: `alpha=0.0001` â†’ `alpha=0.01`
2. Giáº£m sá»‘ neurons: `(128, 64)` â†’ `(64, 32)`
3. Giáº£m sá»‘ layers
4. TÄƒng batch size: `32` â†’ `128`

### Náº¿u bá»‹ Underfitting (Train vÃ  Test Ä‘á»u tháº¥p):

1. TÄƒng sá»‘ neurons vÃ  layers
2. Giáº£m regularization: `alpha=0.001` â†’ `alpha=0.0001`
3. TÄƒng epochs
4. Thá»­ activation function khÃ¡c

---

## ğŸ“ Ghi ChÃº

- **Early Stopping**: MÃ´ hÃ¬nh sáº½ tá»± Ä‘á»™ng dá»«ng náº¿u khÃ´ng cáº£i thiá»‡n sau 10 epochs
- **Validation**: 10% train set Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ validation
- **Random State**: Äáº·t `RANDOM_STATE=42` Ä‘á»ƒ káº¿t quáº£ cÃ³ thá»ƒ tÃ¡i láº­p

---

**ChÃºc báº¡n thá»­ nghiá»‡m thÃ nh cÃ´ng!** ğŸš€
